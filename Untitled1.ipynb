{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa81d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8fb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.utils.data.dataset in torch.utils.data:\n",
      "\n",
      "NAME\n",
      "    torch.utils.data.dataset\n",
      "\n",
      "CLASSES\n",
      "    typing.Generic(builtins.object)\n",
      "        Dataset\n",
      "            ConcatDataset\n",
      "            IterableDataset\n",
      "                ChainDataset\n",
      "            Subset\n",
      "            TensorDataset\n",
      "    \n",
      "    class ChainDataset(IterableDataset)\n",
      "     |  ChainDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  Dataset for chainning multiple :class:`IterableDataset` s.\n",
      "     |  \n",
      "     |  This class is useful to assemble different existing dataset streams. The\n",
      "     |  chainning operation is done on-the-fly, so concatenating large-scale\n",
      "     |  datasets with this class will be efficient.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      datasets (iterable of IterableDataset): datasets to be chained together\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChainDataset\n",
      "     |      IterableDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, datasets: Iterable[torch.utils.data.dataset.Dataset]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __type_class__ = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from IterableDataset:\n",
      "     |  \n",
      "     |  __add__(self, other: torch.utils.data.dataset.Dataset[+T_co])\n",
      "     |  \n",
      "     |  __getattr__(self, attribute_name)\n",
      "     |  \n",
      "     |  __reduce_ex__(self, *args, **kwargs)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from IterableDataset:\n",
      "     |  \n",
      "     |  __init_subclass__ = _dp_init_subclass(*args, **kwargs) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  register_datapipe_as_function(function_name, cls_to_register) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  register_function(function_name, function) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  set_reduce_ex_hook(hook_fn) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IterableDataset:\n",
      "     |  \n",
      "     |  __annotations__ = {'functions': typing.Dict[str, typing.Callable], 're...\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  functions = {'batch': functools.partial(<function IterableDataset.regi...\n",
      "     |  \n",
      "     |  reduce_ex_hook = None\n",
      "     |  \n",
      "     |  type = typing.Generic[+T_co]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __getitem__(self, index) -> +T_co\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class ConcatDataset(Dataset)\n",
      "     |  ConcatDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  Dataset as a concatenation of multiple datasets.\n",
      "     |  \n",
      "     |  This class is useful to assemble different existing datasets.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      datasets (sequence): List of datasets to be concatenated\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConcatDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, datasets: Iterable[torch.utils.data.dataset.Dataset]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  cumsum(sequence)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  cummulative_sizes\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'cumulative_sizes': typing.List[int], 'datasets': t...\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Dataset(typing.Generic)\n",
      "     |  Dataset(*args, **kwds)\n",
      "     |  \n",
      "     |  An abstract class representing a :class:`Dataset`.\n",
      "     |  \n",
      "     |  All datasets that represent a map from keys to data samples should subclass\n",
      "     |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      "     |  data sample for a given key. Subclasses could also optionally overwrite\n",
      "     |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      "     |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      "     |  of :class:`~torch.utils.data.DataLoader`.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      "     |    sampler that yields integral indices.  To make it work with a map-style\n",
      "     |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  __getitem__(self, index) -> +T_co\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class IterableDataset(Dataset)\n",
      "     |  IterableDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  An iterable Dataset.\n",
      "     |  \n",
      "     |  All datasets that represent an iterable of data samples should subclass it.\n",
      "     |  Such form of datasets is particularly useful when data come from a stream.\n",
      "     |  \n",
      "     |  All subclasses should overwrite :meth:`__iter__`, which would return an\n",
      "     |  iterator of samples in this dataset.\n",
      "     |  \n",
      "     |  When a subclass is used with :class:`~torch.utils.data.DataLoader`, each\n",
      "     |  item in the dataset will be yielded from the :class:`~torch.utils.data.DataLoader`\n",
      "     |  iterator. When :attr:`num_workers > 0`, each worker process will have a\n",
      "     |  different copy of the dataset object, so it is often desired to configure\n",
      "     |  each copy independently to avoid having duplicate data returned from the\n",
      "     |  workers. :func:`~torch.utils.data.get_worker_info`, when called in a worker\n",
      "     |  process, returns information about the worker. It can be used in either the\n",
      "     |  dataset's :meth:`__iter__` method or the :class:`~torch.utils.data.DataLoader` 's\n",
      "     |  :attr:`worker_init_fn` option to modify each copy's behavior.\n",
      "     |  \n",
      "     |  Example 1: splitting workload across all workers in :meth:`__iter__`::\n",
      "     |  \n",
      "     |      >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n",
      "     |      ...     def __init__(self, start, end):\n",
      "     |      ...         super(MyIterableDataset).__init__()\n",
      "     |      ...         assert end > start, \"this example code only works with end >= start\"\n",
      "     |      ...         self.start = start\n",
      "     |      ...         self.end = end\n",
      "     |      ...\n",
      "     |      ...     def __iter__(self):\n",
      "     |      ...         worker_info = torch.utils.data.get_worker_info()\n",
      "     |      ...         if worker_info is None:  # single-process data loading, return the full iterator\n",
      "     |      ...             iter_start = self.start\n",
      "     |      ...             iter_end = self.end\n",
      "     |      ...         else:  # in a worker process\n",
      "     |      ...             # split workload\n",
      "     |      ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
      "     |      ...             worker_id = worker_info.id\n",
      "     |      ...             iter_start = self.start + worker_id * per_worker\n",
      "     |      ...             iter_end = min(iter_start + per_worker, self.end)\n",
      "     |      ...         return iter(range(iter_start, iter_end))\n",
      "     |      ...\n",
      "     |      >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
      "     |      >>> ds = MyIterableDataset(start=3, end=7)\n",
      "     |  \n",
      "     |      >>> # Single-process loading\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
      "     |      [3, 4, 5, 6]\n",
      "     |  \n",
      "     |      >>> # Mult-process loading with two worker processes\n",
      "     |      >>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n",
      "     |      [3, 5, 4, 6]\n",
      "     |  \n",
      "     |      >>> # With even more workers\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n",
      "     |      [3, 4, 5, 6]\n",
      "     |  \n",
      "     |  Example 2: splitting workload across all workers using :attr:`worker_init_fn`::\n",
      "     |  \n",
      "     |      >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n",
      "     |      ...     def __init__(self, start, end):\n",
      "     |      ...         super(MyIterableDataset).__init__()\n",
      "     |      ...         assert end > start, \"this example code only works with end >= start\"\n",
      "     |      ...         self.start = start\n",
      "     |      ...         self.end = end\n",
      "     |      ...\n",
      "     |      ...     def __iter__(self):\n",
      "     |      ...         return iter(range(self.start, self.end))\n",
      "     |      ...\n",
      "     |      >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
      "     |      >>> ds = MyIterableDataset(start=3, end=7)\n",
      "     |  \n",
      "     |      >>> # Single-process loading\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
      "     |      [3, 4, 5, 6]\n",
      "     |      >>>\n",
      "     |      >>> # Directly doing multi-process loading yields duplicate data\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n",
      "     |      [3, 3, 4, 4, 5, 5, 6, 6]\n",
      "     |  \n",
      "     |      >>> # Define a `worker_init_fn` that configures each dataset copy differently\n",
      "     |      >>> def worker_init_fn(worker_id):\n",
      "     |      ...     worker_info = torch.utils.data.get_worker_info()\n",
      "     |      ...     dataset = worker_info.dataset  # the dataset copy in this worker process\n",
      "     |      ...     overall_start = dataset.start\n",
      "     |      ...     overall_end = dataset.end\n",
      "     |      ...     # configure the dataset to only process the split workload\n",
      "     |      ...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
      "     |      ...     worker_id = worker_info.id\n",
      "     |      ...     dataset.start = overall_start + worker_id * per_worker\n",
      "     |      ...     dataset.end = min(dataset.start + per_worker, overall_end)\n",
      "     |      ...\n",
      "     |  \n",
      "     |      >>> # Mult-process loading with the custom `worker_init_fn`\n",
      "     |      >>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n",
      "     |      [3, 5, 4, 6]\n",
      "     |  \n",
      "     |      >>> # With even more workers\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n",
      "     |      [3, 4, 5, 6]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IterableDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other: torch.utils.data.dataset.Dataset[+T_co])\n",
      "     |  \n",
      "     |  __getattr__(self, attribute_name)\n",
      "     |  \n",
      "     |  __iter__(self) -> Iterator[+T_co]\n",
      "     |  \n",
      "     |  __reduce_ex__(self, *args, **kwargs)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __init_subclass__ = _dp_init_subclass(*args, **kwargs) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  register_datapipe_as_function(function_name, cls_to_register) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  register_function(function_name, function) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  set_reduce_ex_hook(hook_fn) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'functions': typing.Dict[str, typing.Callable], 're...\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  __type_class__ = False\n",
      "     |  \n",
      "     |  functions = {'batch': functools.partial(<function IterableDataset.regi...\n",
      "     |  \n",
      "     |  reduce_ex_hook = None\n",
      "     |  \n",
      "     |  type = typing.Generic[+T_co]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __getitem__(self, index) -> +T_co\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from torch.utils.data._typing._DataPipeMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Subset(Dataset)\n",
      "     |  Subset(*args, **kwds)\n",
      "     |  \n",
      "     |  Subset of a dataset at specified indices.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      dataset (Dataset): The whole Dataset\n",
      "     |      indices (sequence): Indices in the whole set selected for subset\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Subset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], indices: Sequence[int]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'dataset': torch.utils.data.dataset.Dataset[+T_co],...\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class TensorDataset(Dataset)\n",
      "     |  TensorDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  Dataset wrapping tensors.\n",
      "     |  \n",
      "     |  Each sample will be retrieved by indexing tensors along the first dimension.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      *tensors (Tensor): tensors that have the same size of the first dimension.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TensorDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __init__(self, *tensors: torch.Tensor) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'tensors': typing.Tuple[torch.Tensor, ...]}\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[typing.Tuple[torch....\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "FUNCTIONS\n",
      "    random_split(dataset: torch.utils.data.dataset.Dataset[~T], lengths: Sequence[int], generator: Union[torch._C.Generator, NoneType] = <torch._C.Generator object at 0x0000021850E36B70>) -> List[torch.utils.data.dataset.Subset[~T]]\n",
      "        Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
      "        Optionally fix the generator for reproducible results, e.g.:\n",
      "        \n",
      "        >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
      "        \n",
      "        Args:\n",
      "            dataset (Dataset): Dataset to be split\n",
      "            lengths (sequence): lengths of splits to be produced\n",
      "            generator (Generator): Generator used for the random permutation.\n",
      "    \n",
      "    randperm(...)\n",
      "        randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "        \n",
      "        Returns a random permutation of integers from ``0`` to ``n - 1``.\n",
      "        \n",
      "        Args:\n",
      "            n (int): the upper bound (exclusive)\n",
      "        \n",
      "        Keyword args:\n",
      "            generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: ``torch.int64``.\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "            pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "                the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.randperm(4)\n",
      "            tensor([2, 1, 0, 3])\n",
      "\n",
      "DATA\n",
      "    Callable = typing.Callable\n",
      "    Dict = typing.Dict\n",
      "    Iterable = typing.Iterable\n",
      "    Iterator = typing.Iterator\n",
      "    List = typing.List\n",
      "    Optional = typing.Optional\n",
      "    Sequence = typing.Sequence\n",
      "    T = ~T\n",
      "    T_co = +T_co\n",
      "    Tuple = typing.Tuple\n",
      "    default_generator = <torch._C.Generator object>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\tides\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4b6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab196963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
